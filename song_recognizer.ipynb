{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#******************************************************************************************************************************************************#\n",
    "\"\"\"\n",
    "    @author Shaela Khan, Created 1st April,2022,  Updated : 24th April, Sunday, Last Updated 19th August, Friday 2022.\n",
    "\n",
    "#  song_recognizer.ipynb  -> In Modelling Phase\n",
    "#  Bird Recognition from birdsongs using Deep learning -> We are working on building a model using Deep learning techniques to identify birds from their songs.\n",
    "#  DataSource : - https://www.kaggle.com/datasets/rtatman/british-birdsong-dataset  (Aggregated from the original Xeno-Canto Dataset.)\n",
    "#  Provided dataset has a directory with - ./small Xeno-Canto/songs --> Original audio dataset.\n",
    "#                                        -  ./small Xeno-Canto/birdsong_metadata.csv  --> Original Dataset with labels.\n",
    "                                         -   ./img_data --> audio files converted into image files --> contains spectrographs of audio data.\n",
    "#                                        - traincsv.csv file with labels\n",
    "#                                        - testcsv.csv - for testing model performance\n",
    "#\n",
    "#  We then create a CNN model with possible usage of pre-trained models, that can identify the difference classes defined - this is a supervised learning\n",
    "#  problem.\n",
    "#  Input: birdsong_metadata.csv + songs\n",
    "#  Output : The file should have\n",
    "#          : Classify image from testing provided.\n",
    "\"\"\"\n",
    "#*******************************************************************************************************************************************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# COMMON LIBRARIES\n",
    "import os,sys,random,math\n",
    "from random import seed, random, randint\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "# import import_ipynb,import nbimporter , import helper-functions\n",
    "\n",
    "\n",
    "# FILE PROCESSING\n",
    "import csv\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from shutil import move\n",
    "from itertools import groupby\n",
    "\n",
    "# Plotting AND Visuals.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, shutil, itertools, pickle,seaborn as sn, math, time,scipy\n",
    "\n",
    "# Stats\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "# Image processing libraries\n",
    "#!pip install Pillow\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# EXTRA -\n",
    "%matplotlib inline\n",
    "\n",
    "# ML, statistics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# TENSORFLOW LIBRARIES. BACK-END SUPPORT.\n",
    "# Creating Model and model Evaluation\n",
    "import tensorflow as tf\n",
    "import tensorboard,tensorflow_estimator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "\n",
    "import tensorflow.keras.utils\n",
    "import tensorflow.keras.activations\n",
    "import tensorflow.keras.applications\n",
    "import tensorflow.keras.callbacks\n",
    "import tensorflow.keras.initializers\n",
    "from tensorflow.keras import losses,metrics\n",
    "\n",
    "\n",
    "# Audio file processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# EXTRA EXTRA - because they don't work upstairs?\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Start Code Here.\n",
    "print(\"Hello World ! \")\n",
    "print(\"**********  Sweet Caroline ba ba ba, Shit hehehe!!**********\")\n",
    "print(\"\\n Tensorflow version: \", tf.__version__)\n",
    "\n",
    "# Prints current Python version\n",
    "print(\"Python  \", sys.version)\n",
    "#print(\"\\n Pillow \" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data preprocessing.****************************************************\n",
    "# Read metadata file\n",
    "metadata = pd.read_csv(\"./small Xeno-Canto/birdsong_metadata.csv\") # metadata pandas object Holds all of the data from the csv file.\n",
    "header = list(metadata.head())\n",
    "\n",
    "\n",
    "# Get bird names\n",
    "bird_name = metadata['english_cname'].values\n",
    "u, f = np.unique(bird_name, return_counts=True)\n",
    "print(\"Unique Bird Names  \\n\",u) # Dont touch this part - works\n",
    "print(\"data structure.\")\n",
    "print(metadata.head())\n",
    "print(metadata.info(verbose=True))\n",
    "#print(\"\\n\", bird_name)\n",
    "\n",
    "# helper function, to sort in alphabetical order.\n",
    "#guest_birds = bird_name\n",
    "#guest_birds.sort()\n",
    "#print(\"Hoopla over nothing. \",guest_birds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenize import String\n",
    "# ADDING labels now , from the csv files - pandas provides the df object.\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "tdf = pd.read_csv('./small Xeno-Canto/birdsong_metadata.csv')\n",
    "#print(tdf.head())\n",
    "#print(tdf.info(verbose=True))\n",
    "\n",
    "\n",
    "# Cleanup tdf  tdf  ----> According to proper processing procedures.\n",
    "tdf = tdf.drop(columns='who_provided_recording')\n",
    "tdf = tdf.drop(columns='country')\n",
    "tdf = tdf.drop(columns='latitude')\n",
    "tdf = tdf.drop(columns='longitute')\n",
    "tdf = tdf.drop(columns='license')\n",
    "#print(\"After dropping the redundant columns.\")  #print(tdf.head())\n",
    "# Find a better way to drop multiple columns drop dataframe columns.\n",
    "\n",
    "\n",
    "# For brevity and clarity in processing.\n",
    "tdf['file_id'] = tdf['file_id'].apply(str)\n",
    "\n",
    "\n",
    "# train_df = The one here is a manual hack. Cleaned up the metadata.csv manually.\n",
    "train_df = pd.read_csv('./small Xeno-Canto/birdsong_metadata_modified.csv')\n",
    "train_df.fillna(0, inplace=True)\n",
    "\n",
    "def extension_train_data(x):\n",
    "    return \"xc\"+str(x)+\".png\"\n",
    "\n",
    "# CHANGING the File extensions from .wav to png --> If you remember the originals were audio files.\n",
    "train_df['file_id'] = train_df['file_id'].apply(extension_train_data)\n",
    "print(\"Checking......................\")\n",
    "print(train_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Pre-processing test-dataset and creating a pandas dataframe for ease of IMPLEMENTATION.\n",
    "print(\"\\n Pre-processing test-dataset and creating a pandas dataframe for ease of IMPLEMENTATION. \")\n",
    "sdir = \"./img_data-test\"\n",
    "filepaths=[]\n",
    "labels=[]\n",
    "flist=os.listdir(sdir)\n",
    "\n",
    "for f in flist:\n",
    "    fpath=os.path.join(sdir,f)\n",
    "    filepaths.append(fpath)\n",
    "    index1=f.rfind('-')+1\n",
    "    index2=f.rfind('.')\n",
    "    klass=f[index1:index2]\n",
    "    labels.append(klass)\n",
    "Fseries=pd.Series(filepaths, name='filepaths')\n",
    "Lseries=pd.Series(labels, name='labels')\n",
    "test_df=pd.concat([Fseries, Lseries], axis=1)\n",
    "#print(test_df.head())\n",
    "#print(\"\\n \", labels)\n",
    "print(\"\\nChecking for TEST dataset  ......................\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating data generators.\n",
    "# rescale all pixel values from 0-255, so after this step all our pixel values are in range (0,1)\n",
    "print(\"Create DATA generators for the model.\\n\")\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255,validation_split=0.3) # Training and Validation split 70/30\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator =datagen.flow_from_dataframe(dataframe=train_df,directory='./img_data/',\n",
    "                                             x_col=\"file_id\",\n",
    "                                             y_col=\"english_cname\",\n",
    "                                             class_mode=\"categorical\",\n",
    "                                             target_size=(64,64),\n",
    "                                             validation_split=0.2,\n",
    "                                             subset=\"training\",\n",
    "                                             seed=1337,batch_size=32,shuffle=False)\n",
    "\n",
    "print('Train generator created')\n",
    "val_generator =datagen.flow_from_dataframe(dataframe=train_df,directory='./img_data/',\n",
    "                                           x_col=\"file_id\",\n",
    "                                           y_col=\"english_cname\",\n",
    "                                           class_mode=\"categorical\",\n",
    "                                           target_size=(64,64),subset=\"validation\",\n",
    "                                           seed=42,batch_size=32,shuffle=False)\n",
    "\n",
    "\n",
    "print('Validation generator created')\n",
    "# test_set =test_datagen.flow_from_directory(test_df,target_size=(64, 64),\n",
    "                                          # batch_size=32, class_mode='categorical', shuffle = False)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(test_df,\n",
    "                                                x_col='filepaths',\n",
    "                                                y_col='labels',\n",
    "                                                target_size=(64,64),\n",
    "                                                class_mode='categorical',\n",
    "                                                color_mode='rgb', shuffle=False, batch_size=2)\n",
    "\n",
    "# test_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels',target_size=img_size,\n",
    "                                  # class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Test generator created')\n",
    "print(\"\\n Sanity check Line.----------\")\n",
    "#print(\"Printing labels of the validation dataset: \\n\",val_generator.labels)\n",
    "#print(\"Printing label- names  of the validation dataset: \\n\",val_generator.class_indices.keys())\n",
    "#print(\"\\n These names are needed for report and Presentation.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper function- to be moved to helper-functions.ipynb\n",
    "from datetime import datetime\n",
    "def timer():\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    print(\"now =\", now)\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"date and time =\", dt_string)\n",
    "\n",
    "print(\"\\n DONE.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Model IMPLEMENTATIOM\n",
    "print(\"Creating simple CNN model \\n\")\n",
    "model_simple = tf.keras.models.Sequential()\n",
    "#model_tn = Sequential()\n",
    "\n",
    "input_shape=(64, 64, 3)#1st hidden layer\n",
    "model_simple.add(Conv2D(32, (3, 3), strides=(2, 2), input_shape=input_shape))\n",
    "model_simple.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model_simple.add(Activation('relu'))#2nd hidden layer\n",
    "model_simple.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model_simple.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model_simple.add(Activation('relu'))#3rd hidden layer\n",
    "model_simple.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model_simple.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model_simple.add(Activation('relu'))#Flatten\n",
    "model_simple.add(Flatten())\n",
    "model_simple.add(Dropout(rate=0.5))#Add fully connected layer.\n",
    "model_simple.add(Dense(64))\n",
    "model_simple.add(Activation('relu'))\n",
    "model_simple.add(Dropout(rate=0.5))#Output layer\n",
    "model_simple.add(Dense(88))   # NUMBER OF CLASSES TO PREDICT.\n",
    "model_simple.add(Activation('softmax'))\n",
    "model_simple.summary()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# MODEL Specifications SETUP.\n",
    "\"\"\" Model specifications notes = these specs epochs= 200, batch_size=8 learning rate=0.01, momentum= 0.9 SGD--> accuracy was shit.\"\"\"\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "l1_learning_rate = 0.01\n",
    "decay_rate = l1_learning_rate / EPOCHS\n",
    "momentum = 0.9\n",
    "sgd = SGD(lr=l1_learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "adam = Adam(learning_rate=l1_learning_rate)\n",
    "model_simple.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "print(\"\\n Done. \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Fit the training-set to the model and Train.\n",
    "train_set = train_generator\n",
    "validation_set = val_generator\n",
    "test_set  = test_generator\n",
    "\n",
    "history = model_simple.fit( train_set,validation_data=validation_set,epochs=EPOCHS,batch_size=BATCH_SIZE)\n",
    "# model._reset_compile_cache()  #print(test_set)\n",
    "#model_simple.fit_generator(train_set, steps_per_epoch=100,epochs=50,validation_data=validation_set,validation_steps=50)\n",
    "print(\"\\n Training Done. \")\n",
    "print(\"\\n Metrics: \",history.history)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "print(\"Evaluate on test data\")\n",
    "results = model_simple.evaluate(train_set,test_set, batch_size=128)#OUTPUT [1.704445120342617, 0.33798882681564246]\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "test_set.reset()\n",
    "pred = model_simple.predict(test_set, steps=50, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(\"So, training Data is now processed. Omg! So much work. Anyway -- now we need to Create the test data folder. Pre-process that and then feed it to cnn ANd then pre-trained model . Evaluate etc...\")\n",
    "print(\"\\n Restarted : -----------------------------------------------------------------\")\n",
    "print(\" Creating test dataset complete! Pre-processing done, TIME FOR MODELLING \")\n",
    "timer()\n",
    "print(\"\\n Initial modelling and training complete.\")\n",
    "\n",
    "print(\"***********************************  End of Processing. !!************************************************\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using PRE-Trained network Models.\n",
    "#import keras_efficientnets from keras_efficientnets import EfficientNetB5\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "\n",
    "print(\"Using ResNet50 network Models---------------------------------------------------\")\n",
    "img_size=(224,224)\n",
    "batch_size=16\n",
    "# build a model\n",
    "img_shape=(img_size[0], img_size[1], 3)\n",
    "\"\"\"model_name=\"ResNet50\"\n",
    "base_model=tf.keras.applications.resnet50.ResNet50(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
    "resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "#babyURAModel = tf.keras.applications.EfficientNetB5()\n",
    "\n",
    "\n",
    "base_model=tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max')\n",
    "# Note you are always told NOT to make the base model trainable initially, that is WRONG you get better results leaving it trainable\n",
    "base_model.trainable=True\n",
    "x=base_model.output\n",
    "x=tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
    "x = Dense(1024, kernel_regularizer = tf.keras.regularizers.l2(l = 0.016),activity_regularizer=tf.keras.regularizers.l1(0.006),\n",
    "                bias_regularizer=tf.keras.regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.3, seed=123)(x) # Dropout(rate=0.3, seed=123)(x)\n",
    "x = Dense(128, kernel_regularizer = tf.keras.regularizers.l2(l = 0.016),activity_regularizer=tf.keras.regularizers.l1(0.006),\n",
    "                bias_regularizer=tf.keras.regularizers.l1(0.006) ,activation='relu')(x)\n",
    "x=Dropout(rate=.45, seed=123)(x)\n",
    "output=Dense(class_count, activation='softmax')(x)  # Output Layer= class_count specifies how many classes it's supposed to classify.\n",
    "\n",
    "model=Model(inputs=base_model.input, outputs=output)\n",
    "# OPTIMIZERS\n",
    "lr=.001 # start with this learning rate\n",
    "sgd =tf.keras.optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "#model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary() \"\"\"\n",
    "\n",
    "input_shape = (216,216, 3)\n",
    "resnet_layers = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
    "\n",
    "for layer in resnet_layers.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "dropout_dense_layer = 0.3\n",
    "model = Sequential()\n",
    "model.add(resnet_layers)\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(Dense(256, use_bias=False))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout_dense_layer))\n",
    "model.add(Dense(88, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"-- Model Architecture--\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# EARLY_STOP_PATIENCE must be < NUM_EPOCHS\n",
    "#NUM_EPOCHS = 10\n",
    "#EARLY_STOP_PATIENCE = 3\n",
    "# Early stopping & checkpointing the best model in ../working dir & restoring that as our model for prediction\n",
    "#cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE)\n",
    "#cb_checkpointer = ModelCheckpoint(filepath = './models/best.hdf5', monitor='val_loss',\n",
    "#                                  save_best_only = True, mode = 'auto')\n",
    "epoch10 = 10\n",
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.7),\n",
    "             #EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='./models/best.hdf5', monitor='val_loss', save_best_only=True, mode='auto')]\n",
    "model.compile(loss=\"categorical_crossentropy\",metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs = 100,\n",
    "                    validation_data=val_generator,\n",
    "                    verbose=True,batch_size=32,\n",
    "                    #class_weight=class_weights_dict,\n",
    "                    callbacks=[callbacks])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}