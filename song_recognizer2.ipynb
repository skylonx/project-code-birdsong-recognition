{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#******************************************************************************************************************************************************#\n",
    "\"\"\"\n",
    "    @author Shaela Khan, Created 20th August,2022,  Last Updated 23rd August, Tuesday 2022.\n",
    "\n",
    "#  song_recognizer2.ipynb  -> Using pre-trained models\n",
    "#  Bird Recognition from birdsongs using Deep learning -> We are working on building a model using Deep learning techniques to identify birds from their songs.\n",
    "#  DataSource : - https://www.kaggle.com/datasets/rtatman/british-birdsong-dataset  (Aggregated from the original Xeno-Canto Dataset.)\n",
    "#  Provided dataset has a directory with - ./small Xeno-Canto/songs --> Original audio dataset.\n",
    "#                                        -  ./small Xeno-Canto/birdsong_metadata.csv  --> Original Dataset with labels.\n",
    "#                                        -  ./img_data-test --> audio converted image files for testing\n",
    "                                         -   ./img_data --> audio files converted into image files --> contains spectrographs of audio data.\n",
    "#                                        - traincsv.csv file with labels\n",
    "#                                        - testcsv.csv - for testing model performance\n",
    "#\n",
    "#  We then create a CNN model with possible usage of pre-trained models, that can identify the difference classes defined - this is a supervised learning\n",
    "#  problem.\n",
    "#  Input: birdsong_metadata.csv + songs\n",
    "#  Output : The file should have\n",
    "#          : Classify image from testing provided.\n",
    "\"\"\"\n",
    "#*******************************************************************************************************************************************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# COMMON LIBRARIES\n",
    "import os,sys,random,math\n",
    "from random import seed, random, randint\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "# import import_ipynb,import nbimporter , import helper-functions\n",
    "\n",
    "\n",
    "# FILE PROCESSING\n",
    "import csv\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from shutil import move\n",
    "from itertools import groupby\n",
    "\n",
    "# Plotting AND Visuals.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, shutil, itertools, pickle,seaborn as sn, math, time,scipy\n",
    "\n",
    "# Stats\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "# Image processing libraries\n",
    "#!pip install Pillow\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# EXTRA -\n",
    "%matplotlib inline\n",
    "\n",
    "# ML, statistics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# TENSORFLOW LIBRARIES. BACK-END SUPPORT.\n",
    "# Creating Model and model Evaluation\n",
    "import tensorflow as tf\n",
    "import tensorboard,tensorflow_estimator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "\n",
    "import tensorflow.keras.utils\n",
    "import tensorflow.keras.activations\n",
    "import tensorflow.keras.applications\n",
    "import tensorflow.keras.callbacks\n",
    "import tensorflow.keras.initializers\n",
    "from tensorflow.keras import losses,metrics\n",
    "\n",
    "\n",
    "# Audio file processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "# EXTRA EXTRA - because they don't work upstairs?\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Start Code Here.\n",
    "print(\"Hello World ! \")\n",
    "print(\"**********  Sweet Caroline ba ba ba !!**********\")\n",
    "print(\"\\n Tensorflow version: \", tf.__version__)\n",
    "\n",
    "# Prints current Python version\n",
    "print(\"Python  \", sys.version)\n",
    "#print(\"\\n Pillow \" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data preprocessing.****************************************************\n",
    "# Read metadata file\n",
    "metadata = pd.read_csv(\"./small Xeno-Canto/birdsong_metadata.csv\") # metadata pandas object Holds all of the data from the csv file.\n",
    "header = list(metadata.head())\n",
    "\n",
    "\n",
    "# Get bird names\n",
    "bird_name = metadata['english_cname'].values\n",
    "u, f = np.unique(bird_name, return_counts=True)\n",
    "print(\"Unique Bird Names  \\n\",u) # Dont touch this part - works\n",
    "print(\"data structure.\")\n",
    "print(metadata.head())\n",
    "print(metadata.info(verbose=True))\n",
    "#print(\"\\n\", bird_name)\n",
    "\n",
    "# helper function, to sort in alphabetical order.\n",
    "#guest_birds = bird_name\n",
    "#guest_birds.sort()\n",
    "#print(\"Hoopla over nothing. \",guest_birds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenize import String\n",
    "# ADDING labels now , from the csv files - pandas provides the df object.\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "tdf = pd.read_csv('./small Xeno-Canto/birdsong_metadata.csv')\n",
    "#print(tdf.head())\n",
    "#print(tdf.info(verbose=True))\n",
    "\n",
    "\n",
    "# Cleanup tdf  tdf  ----> According to proper processing procedures.\n",
    "tdf = tdf.drop(columns='who_provided_recording')\n",
    "tdf = tdf.drop(columns='country')\n",
    "tdf = tdf.drop(columns='latitude')\n",
    "tdf = tdf.drop(columns='longitute')\n",
    "tdf = tdf.drop(columns='license')\n",
    "#print(\"After dropping the redundant columns.\")  #print(tdf.head())\n",
    "# Find a better way to drop multiple columns drop dataframe columns.\n",
    "\n",
    "\n",
    "# For brevity and clarity in processing.\n",
    "tdf['file_id'] = tdf['file_id'].apply(str)\n",
    "\n",
    "\n",
    "# train_df = The one here is a manual hack. Cleaned up the metadata.csv manually.\n",
    "train_df = pd.read_csv('./small Xeno-Canto/birdsong_metadata_modified.csv')\n",
    "train_df.fillna(0, inplace=True)\n",
    "\n",
    "def extension_train_data(x):\n",
    "    return \"xc\"+str(x)+\".png\"\n",
    "\n",
    "# CHANGING the File extensions from .wav to png --> If you remember the originals were audio files.\n",
    "train_df['file_id'] = train_df['file_id'].apply(extension_train_data)\n",
    "print(\"Checking training dataset dataframe : ......................\")\n",
    "print(train_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Pre-processing test-dataset and creating a pandas dataframe for ease of IMPLEMENTATION.\n",
    "print(\"\\n Pre-processing test-dataset and creating a pandas dataframe for ease of IMPLEMENTATION. \")\n",
    "sdir = \"./img_data-test\"\n",
    "filepaths=[]\n",
    "labels=[]\n",
    "flist=os.listdir(sdir)\n",
    "\n",
    "for f in flist:\n",
    "    fpath=os.path.join(sdir,f)\n",
    "    filepaths.append(fpath)\n",
    "    index1=f.rfind('-')+1\n",
    "    index2=f.rfind('.')\n",
    "    klass=f[index1:index2]\n",
    "    labels.append(klass)\n",
    "Fseries=pd.Series(filepaths, name='filepaths')\n",
    "Lseries=pd.Series(labels, name='labels')\n",
    "test_df=pd.concat([Fseries, Lseries], axis=1)\n",
    "#print(\"\\n \", labels)\n",
    "#print(\"\\nChecking TEST dataset dataframe:  ......................\")\n",
    "#print(test_df.head())\n",
    "print(\"\\n-------------------------------------------------------------------\")\n",
    "#print (\"train_df length:\", len(train_df), ' test_df length: ', len(test_df))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating data generators.\n",
    "# rescale all pixel values from 0-255, so after this step all our pixel values are in range (0,1)\n",
    "train_df['labels'] = train_df['english_cname'].astype(str)\n",
    "test_df['labels'] = test_df['labels'].astype(str)\n",
    "print(\"Create DATA generators for the model.\\n\")\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255,validation_split=0.3) # Training and Validation split 70/30\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator =datagen.flow_from_dataframe(dataframe=train_df,directory='./img_data/',\n",
    "                                             x_col=\"file_id\",\n",
    "                                             y_col=\"english_cname\",\n",
    "                                             class_mode=\"categorical\",\n",
    "                                             target_size=(254,254), #target_size=(64,64)\n",
    "                                             validation_split=0.2,\n",
    "                                             subset=\"training\",\n",
    "                                             seed=1337,batch_size=32,shuffle=True)\n",
    "\n",
    "print('Train generator created')\n",
    "val_generator =datagen.flow_from_dataframe(dataframe=train_df,directory='./img_data/',\n",
    "                                           x_col=\"file_id\",\n",
    "                                           y_col=\"english_cname\",\n",
    "                                           class_mode=\"categorical\",\n",
    "                                           target_size=(254,254), #target_size=(64,64)\n",
    "                                           subset=\"validation\",\n",
    "                                           seed=42,batch_size=32,shuffle=True)\n",
    "\n",
    "# We are creating a copy of the val-generator to use as TEST_SET. Reason why we are not changing the name is so that, the\n",
    "# The other models and things in motion already don't break.\n",
    "test_generator1 = val_generator\n",
    "print('Validation generator created')\n",
    "# test_set =test_datagen.flow_from_directory(test_df,target_size=(64, 64),\n",
    "                                          # batch_size=32, class_mode='categorical', shuffle = False)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(test_df,\n",
    "                                                x_col='filepaths',\n",
    "                                                y_col='labels',\n",
    "                                                target_size=(254,254), #target_size=(64,64)\n",
    "                                                class_mode='categorical',\n",
    "                                                color_mode='rgb',\n",
    "                                                shuffle=False, batch_size=32)\n",
    "\n",
    "# test_gen=tvgen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels',target_size=img_size,\n",
    "                                  # class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=2)\n",
    "\n",
    "print('Test generator created')\n",
    "print(\"\\n Sanity check Line.----------\")\n",
    "#print(\"Printing labels of the validation dataset: \\n\",val_generator.labels)\n",
    "#print(\"Printing label- names  of the validation dataset: \\n\",val_generator.class_indices.keys())\n",
    "#print(\"\\n These names are needed for report and Presentation.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper function- to be moved to helper-functions.ipynb\n",
    "from datetime import datetime\n",
    "def timer():\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    print(\"now =\", now)\n",
    "    # dd/mm/YY H:M:S\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"date and time =\", dt_string)\n",
    "\n",
    "print(\"\\n DONE.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the ImageGenerated data\n",
    "import cv2\n",
    "train_files=train_generator.filenames\n",
    "classes=list(train_generator.class_indices.keys())\n",
    "class_count=len(classes)     # WE NEED THIS TO TRAIN OUR MODEL. IT COUNTS THE NUMBER OF CLASSES FROM which to classify.\n",
    "labels=train_generator.labels\n",
    "images, labels=next(train_generator)\n",
    "## convert image to RGB\n",
    "image_rgb = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n",
    "#image_rgb = cv2.cvtColor(images, cv2.COLOR_RGB2YCrCb)\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for i in range (len(image_rgb)):\n",
    "    plt.subplot(4,4, i+1)\n",
    "    index=np.argmax(labels[i])\n",
    "    class_name=classes[index]\n",
    "    plt.title(class_name, color='yellow', fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image_rgb[i]/255)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_image_data, train_labels = train_generator.next()\n",
    "test_image_data, test_labels = test_generator1.next()\n",
    "#train_image_data[0]\n",
    "print(\"\\n ImageGenerator created data trainset:  \",train_image_data.shape)\n",
    "print(\"\\n ImageGenerator created data testset:  \",test_image_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Model IMPLEMENTATIOM\n",
    "print(\"Creating simple CNN model \\n\")\n",
    "model_simple = tf.keras.models.Sequential()\n",
    "#model_tn = Sequential()\n",
    "\n",
    "input_shape=(64, 64, 3)#1st hidden layer\n",
    "model_simple.add(Conv2D(32, (3, 3), strides=(2, 2), input_shape=input_shape))\n",
    "model_simple.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model_simple.add(Activation('relu'))#2nd hidden layer\n",
    "model_simple.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model_simple.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model_simple.add(Activation('relu'))#3rd hidden layer\n",
    "model_simple.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model_simple.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
    "model_simple.add(Activation('relu'))#Flatten\n",
    "model_simple.add(Flatten())\n",
    "model_simple.add(Dropout(rate=0.5))#Add fully connected layer.\n",
    "model_simple.add(Dense(64))\n",
    "model_simple.add(Activation('relu'))\n",
    "model_simple.add(Dropout(rate=0.5))#Output layer\n",
    "model_simple.add(Dense(88))   # NUMBER OF CLASSES TO PREDICT.\n",
    "model_simple.add(Activation('softmax'))\n",
    "model_simple.summary()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# MODEL Specifications SETUP.\n",
    "\"\"\" Model specifications notes = these specs epochs= 200, batch_size=8 learning rate=0.01, momentum= 0.9 SGD--> accuracy was bad.\"\"\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "l1_learning_rate = 0.01\n",
    "decay_rate = l1_learning_rate / EPOCHS\n",
    "momentum = 0.9\n",
    "sgd = SGD(lr=l1_learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "adam = Adam(learning_rate=l1_learning_rate)\n",
    "model_simple.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "print(\"\\n Done. \")\n",
    "\n",
    "# Fit the training-set to the model and Train.\n",
    "train_set = train_generator\n",
    "validation_set = val_generator\n",
    "test_set  = test_generator\n",
    "\n",
    "history = model_simple.fit( train_set,validation_data=validation_set,epochs=EPOCHS,batch_size=BATCH_SIZE)\n",
    "# model._reset_compile_cache()  #print(test_set)\n",
    "#model_simple.fit_generator(train_set, steps_per_epoch=100,epochs=50,validation_data=validation_set,validation_steps=50)\n",
    "print(\"\\n Training Done. \")\n",
    "print(\"\\n Metrics: \",history.history)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "print(\"Evaluate on test data\")\n",
    "results = model_simple.evaluate(train_set,test_set, batch_size=128)#OUTPUT [1.704445120342617, 0.33798882681564246]\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "test_set.reset()\n",
    "pred = model_simple.predict(test_set, steps=50, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ANOTHER RESNET50 model--> Think I was doing it wrong?\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "print(\"------------Second Resnet50 model------------------\\n\")\n",
    "# Pre-process for resnet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "def generators(shape, preprocessing):\n",
    "    #Create the training and validation datasets for\n",
    "    #a given image shape.\n",
    "    imgdatagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocessing,\n",
    "        horizontal_flip = True,\n",
    "        validation_split = 0.2,\n",
    "    )\n",
    "    height, width = shape\n",
    "\n",
    "    restrain_generator =imgdatagen.flow_from_dataframe(dataframe=train_df,directory='./img_data/',\n",
    "                                             x_col=\"file_id\",\n",
    "                                             y_col=\"english_cname\",\n",
    "                                             class_mode=\"categorical\",\n",
    "                                             target_size=(height,width), #target_size=(64,64)\n",
    "                                             validation_split=0.2,\n",
    "                                             subset=\"training\",\n",
    "                                             seed=1337,batch_size=30,shuffle=True)\n",
    "\n",
    "    print('Train generator created')\n",
    "\n",
    "    resval_generator =imgdatagen.flow_from_dataframe(dataframe=train_df,directory='./img_data/',\n",
    "                                           x_col=\"file_id\",\n",
    "                                           y_col=\"english_cname\",\n",
    "                                           class_mode=\"categorical\",\n",
    "                                           target_size=(height,width), #target_size=(64,64)\n",
    "                                           subset=\"validation\",\n",
    "                                           seed=42,batch_size=30,shuffle=True)\n",
    "\n",
    "    print('Validation generator created')\n",
    "    return restrain_generator, resval_generator\n",
    "\n",
    "\n",
    "resnet50 = keras.applications.resnet50\n",
    "restrain_dataset, resval_dataset = generators( (224,224), preprocessing=resnet50.preprocess_input)\n",
    "\n",
    "restrain_image_data, restrain_labels = restrain_dataset.next()\n",
    "restest_image_data, restest_labels = resval_dataset.next()\n",
    "#train_image_data[0]\n",
    "print(restrain_image_data.shape)\n",
    "print(restest_image_data.shape)\n",
    "\n",
    "# Model Creation\n",
    "dropout_dense_layer = 0.3\n",
    "conv_model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "for layer in conv_model.layers:\n",
    "    layer.trainable = False\n",
    "x = tf.keras.layers.Flatten()(conv_model.output)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(dropout_dense_layer)(x)\n",
    "predictions = tf.keras.layers.Dense(88, activation='softmax')(x)\n",
    "full_model = tf.keras.models.Model(inputs=conv_model.input, outputs=predictions)\n",
    "full_model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step : 2 Training and running the model.  RESNET50 ---> The right way??\n",
    "rcallback = [ModelCheckpoint(filepath='./models/bestie.hdf5', monitor='val_loss', save_best_only=True, mode='auto')]\n",
    "\n",
    "full_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adamax(lr=0.001),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "\n",
    "history__ = full_model.fit(restrain_dataset,\n",
    "                           validation_data = resval_dataset,\n",
    "                           callbacks=rcallback,\n",
    "                           epochs=1000)\n",
    "timer()\n",
    "full_model.save_weights('resnet50.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "print(\"----------------------------------------------- \\n\")\n",
    "def plot_history(history, yrange):\n",
    "    #Plot loss and accuracy as a function of the epoch,\n",
    "    #for the training and validation datasets.\n",
    "    #\n",
    "    accr = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    lossr = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Get number of epochs\n",
    "    epochs = range(len(accr))\n",
    "\n",
    "    # Plot training and validation accuracy per epoch\n",
    "    plt.plot(epochs, accr)\n",
    "    plt.plot(epochs, val_acc)\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.ylim(yrange)\n",
    "\n",
    "    # Plot training and validation loss per epoch\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, lossr)\n",
    "    plt.plot(epochs, val_loss)\n",
    "    plt.title('Training and validation loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history__, yrange=(0.9,1))\n",
    "#history=model.fit(x=train_generator, epochs=150,verbose=1,validation_data=val_generator,validation_steps=None,batch_size=32,shuffle=False,callbacks=[cb_checkpointer],initial_epoch=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prediction and plotting results.\n",
    "print(\"----------------------------------------------- \\n\")\n",
    "\n",
    "#Declare variables required for prediction case.\n",
    "classes_to_predict = sorted(train_df.english_cname.unique())\n",
    "\n",
    "\n",
    "# Create dataframe of the predictions --taking in fraction of the data slice.\n",
    "preds = full_model.predict(resval_dataset)\n",
    "validation_df = pd.DataFrame(columns=[\"prediction\", \"groundtruth\", \"correct_prediction\"])\n",
    "\n",
    "\n",
    "for pred, groundtruth in zip(preds[:20], resval_dataset.__getitem__(0)[1]):\n",
    "    validation_df = validation_df.append({\"prediction\":classes_to_predict[np.argmax(pred)],\n",
    "                                       \"groundtruth\":classes_to_predict[np.argmax(groundtruth)],\n",
    "                                       \"correct_prediction\":np.argmax(pred)==np.argmax(groundtruth)}, ignore_index=True)\n",
    "\n",
    "print(\"Predictions: \\n\")\n",
    "print(validation_df)\n",
    "\n",
    "\n",
    "# Evaluations Metric\n",
    "#print(\"Evaluation metric:... \\n\")\n",
    "#model.load_weights(\"./models/best.hdf5\")\n",
    "#trainloss, train_acc = model.evaluate(train_generator,verbose=1)\n",
    "#loss, acc = model.evaluate(val_generator,verbose=1)\n",
    "#print (\"-------Accuracy on test set is: \", acc* 100, \"%\",\"Loss: \",loss,\"-----------\")\n",
    "#print (\"-------Accuracy on train set is: \", train_acc* 100, \"%\",\"Loss: \",trainloss*100,\"-----------\")\n",
    "  # from keras.models import load_model , model = load_model('./vgg16_1.h5')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CLASSIFICATION REPORT----------------------------------------\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "\n",
    "print('Classification Report')\n",
    "predicted_class_idx=classes_to_predict[np.argmax(pred)]\n",
    "\n",
    "#target_names = classes_to_predict\n",
    "#print(classification_report(train_generator.classes, predicted_class_idx, target_names=target_names))\n",
    "\n",
    "\n",
    "\n",
    "# Confusion Matrix.\n",
    "#classes_to_predict = np.array(classes_to_predict)\n",
    "#predicted_class_idx = np.array(predicted_class_idx)\n",
    "#cm = confusion_matrix(classes_to_predict, predicted_class_idx)\n",
    "#print('Confusion Matrix')\n",
    "#print(cm)\n",
    "\n",
    "print(\"Show me this is done.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "timer()\n",
    "print(\"***********************************  End of Processing. !!************************************************\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}